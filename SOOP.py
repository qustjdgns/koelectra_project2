import time
import pandas as pd
import numpy as np
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, TimeoutException

# --- [A] ì„¤ì • ë³€ìˆ˜ (Configuration) ---
BASE_URL = "https://sotong.sooplive.co.kr/?board_type=user&work=list&check_nick=false&check_title=true&check_content=true&page="

# âš ï¸ 2ë§Œ ê±´ ì´ìƒ í™•ë³´ë¥¼ ìœ„í•´ MAX_PAGESë¥¼ 2000ìœ¼ë¡œ ì„¤ì • (30,000ê±´ ëª©í‘œ)
MAX_PAGES = 2500
SLEEP_TIME_LIST = 1.5  # ëª©ë¡ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
SLEEP_TIME_DETAIL = 1.0  # ìƒì„¸ í˜ì´ì§€ ì ‘ì† ëŒ€ê¸°

# âš ï¸ ìµœì¢… ìˆ˜ì •ëœ ë³¸ë¬¸ CSS ì„ íƒì (HTML ë¶„ì„ ê²°ê³¼ ë°˜ì˜)
CONTENT_SELECTOR = "div.v_article div.view"

data = []


def setup_driver():
    """Chrome WebDriverë¥¼ ì„¤ì •í•˜ê³  ë°˜í™˜í•œë‹¤."""
    options = webdriver.ChromeOptions()
    options.add_argument("window-size=1920x1080")
    try:
        driver = webdriver.Chrome(options=options)
        return driver
    except Exception as e:
        print(f"WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None


def crawl_list_page(driver, page_num):
    """íŠ¹ì • í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ëª©ë¡ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•œë‹¤."""
    url = BASE_URL + str(page_num)
    driver.get(url)
    time.sleep(SLEEP_TIME_LIST)

    page_data = []
    try:
        post_rows = driver.find_elements(By.CSS_SELECTOR, "#board_list > tr")
    except NoSuchElementException:
        return page_data

    for row in post_rows:
        if 'notice' in row.get_attribute('class'):
            continue

        try:
            cols = row.find_elements(By.TAG_NAME, 'td')
            if len(cols) < 6: continue

            title_element = cols[1].find_element(By.TAG_NAME, 'a')
            detail_link = title_element.get_attribute('href')

            post_data = {
                'post_id': cols[0].text.strip(),
                'title': title_element.text.strip(),
                'author': cols[2].text.strip(),
                'date': cols[3].text.strip(),
                'views': cols[4].text.strip(),
                'recommends': cols[5].text.strip(),
                'detail_url': detail_link
            }
            page_data.append(post_data)

        except Exception:
            continue

    return page_data


def crawl_detail_content(driver, item):
    """ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œë‹¤."""
    try:
        # ìƒì„¸ URLë¡œ ì ‘ì†
        driver.get(item['detail_url'])
        time.sleep(SLEEP_TIME_DETAIL)

        # âš ï¸ ìµœì¢… ìˆ˜ì •ëœ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        content_element = driver.find_element(By.CSS_SELECTOR, CONTENT_SELECTOR)
        item['content'] = content_element.text.strip()

    except NoSuchElementException:
        item['content'] = "ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° ì‹¤íŒ¨"
    except Exception as e:
        item['content'] = f"ìƒì„¸ í˜ì´ì§€ ì ‘ì†/ì¶”ì¶œ ì˜¤ë¥˜: {type(e).__name__}"

    # ëª©ë¡ ë°ì´í„°ì— ì¶”ê°€
    data.append(item)


def main_crawler():
    driver = setup_driver()
    if driver is None: return

    try:
        # MAX_PAGESê¹Œì§€ í¬ë¡¤ë§í•˜ì—¬ 2ë§Œ ê±´ ì´ìƒ ìˆ˜ì§‘ ì‹œë„
        for page_num in range(1, MAX_PAGES + 1):
            print(f"--- ğŸ“š {page_num} í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘ ---")

            # 1. ëª©ë¡ ë°ì´í„° ì¶”ì¶œ
            page_data = crawl_list_page(driver, page_num)

            if not page_data and page_num > 1:
                print("ë” ì´ìƒ ê²Œì‹œê¸€ì´ ì—†ë‹¤. ìˆ˜ì§‘ ì¢…ë£Œí•œë‹¤.")
                break

            # 2. ìƒì„¸ ë³¸ë¬¸ í¬ë¡¤ë§
            for item in page_data:
                crawl_detail_content(driver, item)

            print(f"âœ… í˜ì´ì§€ {page_num}ì—ì„œ {len(page_data)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ. (ì´ {len(data)}ê±´)")

    except Exception as e:
        print(f"\n!!! ì „ì²´ í¬ë¡¤ë§ ì¤‘ë‹¨ ì˜¤ë¥˜: {e} !!!")

    finally:
        driver.quit()
        df = pd.DataFrame(data)

        # âš ï¸ [ë³´ê°•] ìœ íš¨í•˜ì§€ ì•Šì€ 'content'ë¥¼ ê°€ì§„ í–‰ì„ ì œê±°í•˜ì—¬ ìœ íš¨ ë°ì´í„°ë§Œ ì €ì¥
        initial_count = len(df)
        df = df[df['content'] != "ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° ì‹¤íŒ¨"]
        df = df[df['content'].str.strip() != ""]

        removed_count = initial_count - len(df)

        file_name = 'soop_community_data_raw.csv'
        df.to_csv(file_name, index=False, encoding='utf-8-sig')

        print(f"\nâœ¨ ìµœì¢… {initial_count}ê±´ ìˆ˜ì§‘ ì‹œë„. {removed_count}ê±´ì˜ ì˜¤ë¥˜ ë°ì´í„° ì œê±°ë¨.")
        print(f"âœ¨ ìµœì¢… ìœ íš¨ ë°ì´í„° {len(df)}ê±´ '{file_name}'ì— ì €ì¥ë˜ì—ˆë‹¤.")

    return df


# -------------------------------------------------------------------
# 3. ë¼ë²¨ë§ ìƒ˜í”Œ ì¶”ì¶œ í•¨ìˆ˜ (ìˆ˜ì§‘ í›„ ìë™ ì‹¤í–‰)
# -------------------------------------------------------------------

def select_labeling_samples(raw_df):
    """
    ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ë¼ë²¨ë§ì„ ìœ„í•œ ë¬´ì‘ìœ„ ë° ì „ëµì  ìƒ˜í”Œì„ ì¶”ì¶œí•œë‹¤.
    """
    if raw_df.empty:
        print("ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆì–´ ìƒ˜í”Œë§ì„ ì§„í–‰í•  ìˆ˜ ì—†ë‹¤.")
        return

    SAMPLE_FILE = 'soop_data_labeling_sample.csv'
    TOTAL_SAMPLE_SIZE = 2000
    RANDOM_SAMPLE_SIZE = 1000
    STRATEGIC_SAMPLE_SIZE = 1000

    print(f"\n--- ğŸ“ ë¼ë²¨ë§ ìƒ˜í”Œ ì¶”ì¶œ ì‹œì‘ (ì´ {len(raw_df)}ê±´) ---")

    # 1. ë°ì´í„° í´ë¦¬ë‹ ë° ìˆ«ìí˜• ë³€í™˜
    raw_df['views'] = pd.to_numeric(raw_df['views'], errors='coerce').fillna(0).astype(int)
    raw_df['recommends'] = pd.to_numeric(raw_df['recommends'], errors='coerce').fillna(0).astype(int)

    raw_df.drop_duplicates(subset=['post_id'], keep='first', inplace=True)

    # 2. ê·¸ë£¹ A: ë¬´ì‘ìœ„ ìƒ˜í”Œ ì¶”ì¶œ
    n_random = min(RANDOM_SAMPLE_SIZE, len(raw_df))
    random_sample = raw_df.sample(n=n_random, random_state=42)

    # 3. ê·¸ë£¹ B: ì „ëµì  ìƒ˜í”Œ ì¶”ì¶œ (ë°˜ì‘ë„ ê¸°ì¤€)
    df_temp = raw_df.drop(random_sample.index, errors='ignore').copy()

    df_temp['engagement'] = df_temp['views'] + df_temp['recommends']
    engagement_threshold = df_temp['engagement'].quantile(0.9) if len(df_temp) > 0 else 0

    strategic_candidates = df_temp[
        (df_temp['engagement'] >= engagement_threshold) |
        (df_temp['recommends'] > df_temp['views'] / 100)
        ]

    n_strategic = min(STRATEGIC_SAMPLE_SIZE, len(strategic_candidates))
    strategic_sample = strategic_candidates.sample(n=n_strategic, random_state=42)

    # 4. ìµœì¢… ë°ì´í„°ì…‹ í†µí•© ë° ì €ì¥
    final_sample = pd.concat([random_sample, strategic_sample]).drop_duplicates(subset=['post_id'])

    final_sample['label'] = None

    print(f"âœ… ë¬´ì‘ìœ„ ìƒ˜í”Œ: {len(random_sample)}ê±´")
    print(f"âœ… ì „ëµì  ìƒ˜í”Œ: {len(strategic_sample)}ê±´")
    print(f"âœ… ìµœì¢… ë¼ë²¨ë§ ëŒ€ìƒ: {len(final_sample)}ê±´ (ìµœì†Œ {TOTAL_SAMPLE_SIZE}ê±´ ëª©í‘œ)")

    final_sample = final_sample[['post_id', 'title', 'content', 'views', 'recommends', 'date', 'detail_url', 'label']]
    final_sample.to_csv(SAMPLE_FILE, index=False, encoding='utf-8-sig')
    print(f"\nğŸ”¥ ë¼ë²¨ë§ ëŒ€ìƒ íŒŒì¼ '{SAMPLE_FILE}' ì €ì¥ ì™„ë£Œ.")


# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    crawled_df = main_crawler()
    select_labeling_samples(crawled_df)
